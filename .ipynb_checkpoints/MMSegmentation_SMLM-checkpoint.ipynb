{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT_ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAE_h7XhPT7d",
    "outputId": "ce167a21-d10d-4edb-9eeb-0c5f4b60885a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1 True\n",
      "/home/smlm-workstation/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "# import mmseg, math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import mmcv\n",
    "import torch, torchvision\n",
    "\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "# print(mmseg.__version__)\n",
    "\n",
    "# %cd /home/smlm-workstation/segmentation/mmsegmentation/\n",
    "%cd /home/smlm-workstation/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation\n",
    "\n",
    "# split train/val set randomly\n",
    "img_dir = 'images'\n",
    "ann_dir = 'bit_masks'\n",
    "\n",
    "data_root = '/home/smlm-workstation/segmentation/data/full_combined_mt_det_er_summed/'\n",
    "# data_root = '/home/smlm-workstation/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation'\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_dir = 'splits'\n",
    "# mmcv.mkdir_or_exist(osp.join(data_root, split_dir))\n",
    "# filename_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
    "#     osp.join(data_root, img_dir), suffix='.png')]\n",
    "# with open(osp.join(data_root, split_dir, 'train.txt'), 'w') as f:\n",
    "#   # select first 4/5 as train set\n",
    "#   train_length = int(len(filename_list)*99/100)\n",
    "#   f.writelines(line + '\\n' for line in filename_list[:train_length])\n",
    "# with open(osp.join(data_root, split_dir, 'val.txt'), 'w') as f:\n",
    "#   # select last 1/5 as test set\n",
    "#   f.writelines(line + '\\n' for line in filename_list[train_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LbsWOw62_o-X"
   },
   "outputs": [],
   "source": [
    "# from mmseg.datasets.builder import DATASETS\n",
    "# from mmseg.datasets.custom import CustomDataset\n",
    "\n",
    "# @DATASETS.register_module()\n",
    "# class SMLM_mt_ves(CustomDataset):\n",
    "#   CLASSES = ('Background','Microtubule', 'ER')\n",
    "#   PALETTE = [[40,40,40], [128, 255, 0], [255, 255, 128]]\n",
    "#   def __init__(self, split, **kwargs):\n",
    "#     super().__init__(img_suffix='.png', seg_map_suffix='.png', \n",
    "#                      split=split,\n",
    "#                      reduce_zero_label=False,\n",
    "#                      **kwargs)\n",
    "#     assert osp.exists(self.img_dir) and self.split is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets.builder import DATASETS\n",
    "from mmseg.datasets.custom import CustomDataset\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class SMLM_mt_ER(CustomDataset):\n",
    "  CLASSES = ('Microtubule', 'ER')\n",
    "  PALETTE = [[128, 255, 0], [255, 255, 128]]\n",
    "\n",
    "  def __init__(self, split, **kwargs):\n",
    "    super().__init__(img_suffix='.png', seg_map_suffix='.png',\n",
    "                     split=split,\n",
    "                     reduce_zero_label=True,\n",
    "                     **kwargs)\n",
    "    assert osp.exists(self.img_dir) and self.split is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Osxb6QvVtJlX"
   },
   "outputs": [],
   "source": [
    "# !wget https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b2_512x512_160k_ade20k/segformer_mit-b2_512x512_160k_ade20k_20220620_114047-64e4feca.pth -P /home/smlm-workstation/segmentation/mmsegmentation/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyKnYC1Z7iCV",
    "outputId": "4da726b9-ac67-4ec7-b94b-5ed5763e7d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True)\n",
      "model = dict(\n",
      "    type='EncoderDecoder',\n",
      "    pretrained=\n",
      "    '../mae-main/output/millionAID_224/1600_0.75_0.00015_0.05_2048/checkpoint-1599.pth',\n",
      "    backbone=dict(\n",
      "        type='ViT_Win_RVSA_V3_KVDIFF_WSZ7',\n",
      "        img_size=512,\n",
      "        embed_dim=768,\n",
      "        depth=12,\n",
      "        num_heads=12,\n",
      "        mlp_ratio=4,\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.1,\n",
      "        use_abs_pos_emb=True,\n",
      "        patch_size=16,\n",
      "        out_indices=[3, 5, 7, 11]),\n",
      "    decode_head=dict(\n",
      "        type='UPerHead',\n",
      "        in_channels=[768, 768, 768, 768],\n",
      "        in_index=[0, 1, 2, 3],\n",
      "        pool_scales=(1, 2, 3, 6),\n",
      "        channels=512,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=2,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "        ignore_index=0),\n",
      "    auxiliary_head=dict(\n",
      "        type='FCNHead',\n",
      "        in_channels=768,\n",
      "        in_index=2,\n",
      "        channels=256,\n",
      "        num_convs=1,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=2,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4),\n",
      "        ignore_index=0),\n",
      "    train_cfg=dict(),\n",
      "    test_cfg=dict(mode='slide', stride=(384, 384), crop_size=(512, 512)))\n",
      "log_config = dict(\n",
      "    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/vitae_rvsa_Potsdam_91.22.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "cudnn_benchmark = True\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=6e-05,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.05,\n",
      "    constructor='LayerDecayOptimizerConstructor',\n",
      "    paramwise_cfg=dict(num_layers=12, layer_decay_rate=0.9))\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='poly',\n",
      "    warmup='linear',\n",
      "    warmup_iters=1500,\n",
      "    warmup_ratio=1e-06,\n",
      "    power=1.0,\n",
      "    min_lr=0.0,\n",
      "    by_epoch=False)\n",
      "runner = dict(type='IterBasedRunner', max_iters=2000)\n",
      "checkpoint_config = dict(by_epoch=False, interval=500)\n",
      "evaluation = dict(interval=500, metric=['mIoU', 'mFscore'], pre_eval=True)\n",
      "dataset_type = 'SMLM_mt_ER'\n",
      "data_root = '/home/smlm-workstation/segmentation/data/full_combined_mt_det_er_summed/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "crop_size = (512, 512)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', reduce_zero_label=True),\n",
      "    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.9),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=False),\n",
      "    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(512, 512),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=512),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=None,\n",
      "        img_ratios=[1.0],\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=False),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=12,\n",
      "    train=dict(\n",
      "        type='SMLM_mt_ER',\n",
      "        data_root=\n",
      "        '/home/smlm-workstation/segmentation/data/full_combined_mt_det_er_summed/',\n",
      "        img_dir='images',\n",
      "        ann_dir='bit_masks',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', reduce_zero_label=True),\n",
      "            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.9),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "        ],\n",
      "        split='splits/train.txt'),\n",
      "    val=dict(\n",
      "        type='SMLM_mt_ER',\n",
      "        data_root=\n",
      "        '/home/smlm-workstation/segmentation/data/full_combined_mt_det_er_summed/',\n",
      "        img_dir='images',\n",
      "        ann_dir='bit_masks',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=None,\n",
      "                img_ratios=[1.0],\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='splits/val.txt'),\n",
      "    test=dict(\n",
      "        type='SMLM_mt_ER',\n",
      "        data_root=\n",
      "        '/home/smlm-workstation/segmentation/data/full_combined_mt_det_er_summed/',\n",
      "        img_dir='images',\n",
      "        ann_dir='bit_masks',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=None,\n",
      "                img_ratios=[1.0],\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='splits/val.txt'))\n",
      "reduce_zero_label = True\n",
      "work_dir = './work_dirs/ViTAE_UperNet_Potsdam_512_MT_ER'\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "device = 'cuda'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from distutils.fancy_getopt import FancyGetopt\n",
    "from mmseg.apis import set_random_seed\n",
    "from mmcv import Config\n",
    "\n",
    "# cfg = Config.fromfile(\n",
    "#     'configs/vit_base_win/upernet_vitae_nc_base_rvsa_v3_wsz7_512x512_160k_potsdam_rgb_dpr10_lr6e5_lrd90_ps16_class5_ignore5.py')\n",
    "\n",
    "cfg = Config.fromfile(\n",
    "    'configs/vit_base_win/upernet_vit_base_win_rvsa_v3_kvdiff_512x512_160k_potsdam_rgb_dpr10_lr6e5_lrd90_ps16_class5_ignore5.py')\n",
    "# cfg = Config.fromfile(\n",
    "# 'configs/segformer/segformer_mit-b1_512x512_160k_ade20k.py')\n",
    "\n",
    "# Since we use only one GPU, BN is used instead of SyncBN\n",
    "cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "# cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "# modify num classes of the model in decode/auxiliary head\n",
    "cfg.model.decode_head.num_classes = 2\n",
    "cfg.model.auxiliary_head.num_classes = 2\n",
    "# cfg.model.pretrained = 'checkpoints/vitae-b-checkpoint-1599-transform-no-average.pth'\n",
    "\n",
    "# cfg.model.test_cfg = dotdict(\n",
    "#     mode='slide', crop_size=(256, 256), stride=(200, 200))\n",
    "\n",
    "# cfg.model.test_cfg = dotdict(mode='slide', crop_size=(128, 128), stride=(127, 127))\n",
    "# cfg.model.auxiliary_head.num_classes = 3\n",
    "\n",
    "# cfg.model.test_cfg = dotdict(mode='slide', crop_size=(256, 256), stride=(1, 1))\n",
    "# cfg.model.test_cfg = dotdict(\n",
    "#     mode='whole')\n",
    "\n",
    "# cfg.model.auxiliary_head.loss_decode = dict(\n",
    "#     type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0, avg_non_ignore=True, class_weight=[0.05, 0.55, 0.45])\n",
    "# cfg.model.decode_head.loss_decode = [dict(type='CrossEntropyLoss', loss_name='loss_ce', loss_weight=1.0),\n",
    "#                                      dict(type='DiceLoss', use_sigmoid=False, loss_weight=1.0)]\n",
    "                                    #  dict(type='TverskyLoss', loss_name='TverskyLoss', loss_weight=3.0)]\n",
    "\n",
    "# cfg.model.decode_head.loss_decode = dict(type='CrossEntropyLoss', use_sigmoid=False, class_weight=[0.3504681, 0.6460288])\n",
    "# cfg.model.decode_head.loss_decode = dict(type='DiceLoss', use_sigmoid=True)\n",
    "# cfg.model.decode_head.loss_decode = dict(type='DiceLoss', use_sigmoid=True, class_weight=[0.35, 0.64])\n",
    "# cfg.model.decode_head.loss_decode = [dict(type='FocalLoss', use_sigmoid=True, alpha=.25)]\n",
    "\n",
    "# cfg.model.decode_head.loss_decode = [dict(type='FocalLoss', use_sigmoid=True, alpha=.25, loss_weight=4., class_weight=[0.0035031103, 0.3504681, 0.6460288]),\n",
    "#                                      dict(type='DiceLoss', loss_name='dice', loss_weight=1., class_weight=[0.0035031103, 0.3504681, 0.6460288])]\n",
    "\n",
    "# cfg.model.decode_head.loss_decode = dict(\n",
    "#     type='PhiLoss', loss_weight=1.0, gamma=0.5)\n",
    "\n",
    "# cfg.model.decode_head.loss_decode = dict(\n",
    "#     type='TverskyLoss', class_weight=[0.2, 0.3, 0.5])\n",
    "\n",
    "cfg.model.auxiliary_head.ignore_index = 0\n",
    "cfg.model.decode_head.ignore_index = 0\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'SMLM_mt_ER'\n",
    "cfg.data_root = data_root\n",
    "cfg.reduce_zero_label = True\n",
    "\n",
    "cfg.data.samples_per_gpu = 4\n",
    "cfg.data.workers_per_gpu = 12\n",
    "\n",
    "img_norm_cfg = dict(\n",
    "mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=False)\n",
    "crop_size = (512, 512)\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', reduce_zero_label=True),\n",
    "    # dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),\n",
    "    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.9),\n",
    "    # dict(type='RandomRotate', prob=0.5, degree=35),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='PhotoMetricDistortion'),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
    "]\n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=None,\n",
    "        img_ratios=[1.0],\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(type='Normalize', **img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img'])\n",
    "        ])\n",
    "]\n",
    "\n",
    "cfg.data.train.type = cfg.dataset_type\n",
    "cfg.data.train.data_root = cfg.data_root\n",
    "# cfg.data.train.reduce_zero_label = cfg.reduce_zero_label\n",
    "cfg.data.train.img_dir = img_dir\n",
    "cfg.data.train.ann_dir = ann_dir\n",
    "cfg.data.train.pipeline = cfg.train_pipeline\n",
    "cfg.data.train.split = 'splits/train.txt'\n",
    "\n",
    "cfg.data.val.type = cfg.dataset_type\n",
    "cfg.data.val.data_root = cfg.data_root\n",
    "# cfg.data.val.reduce_zero_label = cfg.reduce_zero_label\n",
    "cfg.data.val.img_dir = img_dir\n",
    "cfg.data.val.ann_dir = ann_dir\n",
    "cfg.data.val.pipeline = cfg.test_pipeline\n",
    "cfg.data.val.split = 'splits/val.txt'\n",
    "\n",
    "cfg.data.test.type = cfg.dataset_type\n",
    "cfg.data.test.data_root = cfg.data_root\n",
    "# cfg.data.test.reduce_zero_label = cfg.reduce_zero_label\n",
    "cfg.data.test.img_dir = img_dir\n",
    "cfg.data.test.ann_dir = ann_dir\n",
    "cfg.data.test.pipeline = cfg.test_pipeline\n",
    "cfg.data.test.split = 'splits/val.txt'\n",
    "\n",
    "# cfg.log_config = dict(  \n",
    "#     interval=50,  \n",
    "#     hooks=[\n",
    "#         dict(type='TextLoggerHook', by_epoch=False),\n",
    "#         # dict(type='TensorboardLoggerHook', by_epoch=False),\n",
    "#         # dict(type='NeptuneLoggerHook', by_epoch=False) \n",
    "#         # MMSegWandbHook is mmseg implementation of WandbLoggerHook. ClearMLLoggerHook, DvcliveLoggerHook, MlflowLoggerHook, NeptuneLoggerHook, PaviLoggerHook, SegmindLoggerHook are also supported based on MMCV implementation.\n",
    "#     ])\n",
    "cfg.log_config.interval = 50\n",
    "cfg.runner.max_iters = 2000\n",
    "cfg.evaluation.interval = 500\n",
    "cfg.checkpoint_config.interval = 500\n",
    "\n",
    "# cfg.resume_from = 'work_dirs/segformer_b1_adamW_16k/iter_16000.pth'\n",
    "# cfg.load_from = 'work_dirs/segformer_b1_adamW_16k/iter_16000.pth'\n",
    "# cfg.load_from = 'checkpoints/vitae_rvsa_Potsdam_91.22.pth'\n",
    "cfg.load_from = 'checkpoints/vit_rvsa_kvdiff_Potsdam_90.77.pth'\n",
    "# cfg.load_from = 'checkpoints/segformer_mit-b1_512x512_160k_ade20k_20220620_112037-c3f39e00.pth'\n",
    "# cfg.load_from = 'checkpoints/segformer_mit-b2_512x512_160k_ade20k_20220620_114047-64e4feca.pth'\n",
    "# cfg.load_from = 'checkpoints/deeplabv3plus_r18-d8_512x512_80k_potsdam_20211219_020601-75fd5bc3.pth'\n",
    "# cfg.load_from = 'checkpoints/deeplabv3plus_r50-d8_4x4_512x512_80k_vaihingen_20211231_230816-5040938d.pth'\n",
    "# cfg.load_from = 'checkpoints/deeplabv3plus_r101-d8_512x512_80k_potsdam_20211219_031508-8b112708.pth'\n",
    "# cfg.load_from = 'checkpoints/deeplabv3plus_r101-d8_4x4_512x512_80k_vaihingen_20211231_230816-8a095afa.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './work_dirs/ViTAE_UperNet_Potsdam_512_MT_ER'\n",
    "\n",
    "# optimizer = dict(\n",
    "#     _delete_=True,\n",
    "#     type='AdamW',\n",
    "#     lr=0.000005,\n",
    "#     betas=(0.9, 0.999),\n",
    "#     weight_decay=0.01,\n",
    "#     paramwise_cfg=dict(\n",
    "#         custom_keys={\n",
    "#             'pos_block': dict(decay_mult=0.),\n",
    "#             'norm': dict(decay_mult=0.),\n",
    "#             'head': dict(lr_mult=10.)\n",
    "#         }))\n",
    "\n",
    "# lr_config = dict(\n",
    "#     _delete_=True,\n",
    "#     policy='poly',\n",
    "#     warmup='linear',\n",
    "#     warmup_iters=500,\n",
    "#     warmup_ratio=1e-6,\n",
    "#     power=1.0,\n",
    "#     min_lr=0.0,\n",
    "#     by_epoch=False)\n",
    "\n",
    "# cfg.optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0005,\n",
    "#                      paramwise_cfg=dict(custom_keys={'head': dict(lr_mult=10.)}))\n",
    "\n",
    "# cfg.optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
    "\n",
    "# cfg.lr_config=dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(10, 1e-5),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "# cfg.momentum_config=dict(\n",
    "#     policy='cyclic',\n",
    "#     target_ratio=(0.85 / 0.95, 1),\n",
    "#     cyclic_times=1,\n",
    "#     step_ratio_up=0.4,\n",
    "# )\n",
    "\n",
    "# Set seed to facitate reproducing the result\n",
    "cfg.seed = 42\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.device = 'cuda'\n",
    "cfg.cudnn_benchmark = True\n",
    "\n",
    "# Let's have a look at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYKoSfdMF12B",
    "outputId": "3cf9d4f1-3a9b-4129-f3df-684829973d02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 12:29:55,463 - mmseg - INFO - Loaded 1663 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smlm-workstation/miniconda3/envs/vitae/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n",
      "The relative_pos_embedding is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 12:29:58,366 - mmseg - INFO - Loaded 17 images\n",
      "2022-11-24 12:29:58,367 - mmseg - INFO - load checkpoint from local path: checkpoints/vitae_rvsa_Potsdam_91.22.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 12, 'layer_decay_rate': 0.9}\n",
      "Build LayerDecayOptimizerConstructor 0.900000 - 14\n",
      "Param groups = {\n",
      "  \"layer_0_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.pos_embed\",\n",
      "      \"backbone.patch_embed.proj.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001,\n",
      "    \"lr\": 1.5251194969974005e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_0_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.patch_embed.proj.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001,\n",
      "    \"lr\": 1.5251194969974005e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_1_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.0.norm1.weight\",\n",
      "      \"backbone.blocks.0.norm1.bias\",\n",
      "      \"backbone.blocks.0.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.0.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.0.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.0.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.0.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.0.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.0.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.0.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.0.attn.qkv.bias\",\n",
      "      \"backbone.blocks.0.attn.proj.bias\",\n",
      "      \"backbone.blocks.0.norm2.weight\",\n",
      "      \"backbone.blocks.0.norm2.bias\",\n",
      "      \"backbone.blocks.0.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.0.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001,\n",
      "    \"lr\": 1.694577218886001e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_1_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.0.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.0.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.0.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.0.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.0.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.0.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.0.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.0.attn.qkv.weight\",\n",
      "      \"backbone.blocks.0.attn.proj.weight\",\n",
      "      \"backbone.blocks.0.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.0.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001,\n",
      "    \"lr\": 1.694577218886001e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_2_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.1.norm1.weight\",\n",
      "      \"backbone.blocks.1.norm1.bias\",\n",
      "      \"backbone.blocks.1.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.1.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.1.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.1.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.1.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.1.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.1.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.1.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.1.attn.qkv.bias\",\n",
      "      \"backbone.blocks.1.attn.proj.bias\",\n",
      "      \"backbone.blocks.1.norm2.weight\",\n",
      "      \"backbone.blocks.1.norm2.bias\",\n",
      "      \"backbone.blocks.1.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.1.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006,\n",
      "    \"lr\": 1.8828635765400005e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_2_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.1.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.1.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.1.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.1.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.1.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.1.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.1.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.1.attn.qkv.weight\",\n",
      "      \"backbone.blocks.1.attn.proj.weight\",\n",
      "      \"backbone.blocks.1.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.1.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006,\n",
      "    \"lr\": 1.8828635765400005e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_3_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.2.norm1.weight\",\n",
      "      \"backbone.blocks.2.norm1.bias\",\n",
      "      \"backbone.blocks.2.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.2.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.2.attn.qkv.bias\",\n",
      "      \"backbone.blocks.2.attn.proj.bias\",\n",
      "      \"backbone.blocks.2.norm2.weight\",\n",
      "      \"backbone.blocks.2.norm2.bias\",\n",
      "      \"backbone.blocks.2.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.2.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001,\n",
      "    \"lr\": 2.0920706406000006e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_3_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.2.attn.qkv.weight\",\n",
      "      \"backbone.blocks.2.attn.proj.weight\",\n",
      "      \"backbone.blocks.2.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.2.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001,\n",
      "    \"lr\": 2.0920706406000006e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_4_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.3.norm1.weight\",\n",
      "      \"backbone.blocks.3.norm1.bias\",\n",
      "      \"backbone.blocks.3.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.3.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.3.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.3.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.3.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.3.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.3.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.3.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.3.attn.qkv.bias\",\n",
      "      \"backbone.blocks.3.attn.proj.bias\",\n",
      "      \"backbone.blocks.3.norm2.weight\",\n",
      "      \"backbone.blocks.3.norm2.bias\",\n",
      "      \"backbone.blocks.3.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.3.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001,\n",
      "    \"lr\": 2.3245229340000006e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_4_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.3.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.3.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.3.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.3.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.3.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.3.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.3.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.3.attn.qkv.weight\",\n",
      "      \"backbone.blocks.3.attn.proj.weight\",\n",
      "      \"backbone.blocks.3.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.3.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001,\n",
      "    \"lr\": 2.3245229340000006e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_5_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.4.norm1.weight\",\n",
      "      \"backbone.blocks.4.norm1.bias\",\n",
      "      \"backbone.blocks.4.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.4.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.4.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.4.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.4.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.4.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.4.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.4.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.4.attn.qkv.bias\",\n",
      "      \"backbone.blocks.4.attn.proj.bias\",\n",
      "      \"backbone.blocks.4.norm2.weight\",\n",
      "      \"backbone.blocks.4.norm2.bias\",\n",
      "      \"backbone.blocks.4.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.4.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001,\n",
      "    \"lr\": 2.5828032600000008e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_5_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.4.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.4.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.4.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.4.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.4.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.4.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.4.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.4.attn.qkv.weight\",\n",
      "      \"backbone.blocks.4.attn.proj.weight\",\n",
      "      \"backbone.blocks.4.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.4.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001,\n",
      "    \"lr\": 2.5828032600000008e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_6_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.5.norm1.weight\",\n",
      "      \"backbone.blocks.5.norm1.bias\",\n",
      "      \"backbone.blocks.5.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.5.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.5.attn.qkv.bias\",\n",
      "      \"backbone.blocks.5.attn.proj.bias\",\n",
      "      \"backbone.blocks.5.norm2.weight\",\n",
      "      \"backbone.blocks.5.norm2.bias\",\n",
      "      \"backbone.blocks.5.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.5.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001,\n",
      "    \"lr\": 2.8697814000000007e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_6_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.5.attn.qkv.weight\",\n",
      "      \"backbone.blocks.5.attn.proj.weight\",\n",
      "      \"backbone.blocks.5.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.5.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001,\n",
      "    \"lr\": 2.8697814000000007e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_7_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.6.norm1.weight\",\n",
      "      \"backbone.blocks.6.norm1.bias\",\n",
      "      \"backbone.blocks.6.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.6.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.6.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.6.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.6.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.6.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.6.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.6.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.6.attn.qkv.bias\",\n",
      "      \"backbone.blocks.6.attn.proj.bias\",\n",
      "      \"backbone.blocks.6.norm2.weight\",\n",
      "      \"backbone.blocks.6.norm2.bias\",\n",
      "      \"backbone.blocks.6.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.6.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441,\n",
      "    \"lr\": 3.1886460000000004e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_7_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.6.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.6.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.6.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.6.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.6.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.6.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.6.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.6.attn.qkv.weight\",\n",
      "      \"backbone.blocks.6.attn.proj.weight\",\n",
      "      \"backbone.blocks.6.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.6.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441,\n",
      "    \"lr\": 3.1886460000000004e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_8_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.7.norm1.weight\",\n",
      "      \"backbone.blocks.7.norm1.bias\",\n",
      "      \"backbone.blocks.7.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.7.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.7.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.7.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.7.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.7.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.7.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.7.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.7.attn.qkv.bias\",\n",
      "      \"backbone.blocks.7.attn.proj.bias\",\n",
      "      \"backbone.blocks.7.norm2.weight\",\n",
      "      \"backbone.blocks.7.norm2.bias\",\n",
      "      \"backbone.blocks.7.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.7.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001,\n",
      "    \"lr\": 3.54294e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_8_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.7.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.7.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.7.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.7.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.7.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.7.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.7.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.7.attn.qkv.weight\",\n",
      "      \"backbone.blocks.7.attn.proj.weight\",\n",
      "      \"backbone.blocks.7.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.7.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001,\n",
      "    \"lr\": 3.54294e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_9_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.8.norm1.weight\",\n",
      "      \"backbone.blocks.8.norm1.bias\",\n",
      "      \"backbone.blocks.8.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.8.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.8.attn.qkv.bias\",\n",
      "      \"backbone.blocks.8.attn.proj.bias\",\n",
      "      \"backbone.blocks.8.norm2.weight\",\n",
      "      \"backbone.blocks.8.norm2.bias\",\n",
      "      \"backbone.blocks.8.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.8.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561,\n",
      "    \"lr\": 3.9366e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_9_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.8.attn.qkv.weight\",\n",
      "      \"backbone.blocks.8.attn.proj.weight\",\n",
      "      \"backbone.blocks.8.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.8.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561,\n",
      "    \"lr\": 3.9366e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_10_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.9.norm1.weight\",\n",
      "      \"backbone.blocks.9.norm1.bias\",\n",
      "      \"backbone.blocks.9.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.9.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.9.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.9.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.9.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.9.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.9.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.9.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.9.attn.qkv.bias\",\n",
      "      \"backbone.blocks.9.attn.proj.bias\",\n",
      "      \"backbone.blocks.9.norm2.weight\",\n",
      "      \"backbone.blocks.9.norm2.bias\",\n",
      "      \"backbone.blocks.9.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.9.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001,\n",
      "    \"lr\": 4.3740000000000005e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_10_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.9.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.9.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.9.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.9.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.9.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.9.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.9.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.9.attn.qkv.weight\",\n",
      "      \"backbone.blocks.9.attn.proj.weight\",\n",
      "      \"backbone.blocks.9.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.9.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001,\n",
      "    \"lr\": 4.3740000000000005e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_11_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.10.norm1.weight\",\n",
      "      \"backbone.blocks.10.norm1.bias\",\n",
      "      \"backbone.blocks.10.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.10.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.10.attn.sampling_offsets_k.2.bias\",\n",
      "      \"backbone.blocks.10.attn.sampling_scales_k.2.bias\",\n",
      "      \"backbone.blocks.10.attn.sampling_angles_k.2.bias\",\n",
      "      \"backbone.blocks.10.attn.sampling_offsets_v.2.bias\",\n",
      "      \"backbone.blocks.10.attn.sampling_scales_v.2.bias\",\n",
      "      \"backbone.blocks.10.attn.sampling_angles_v.2.bias\",\n",
      "      \"backbone.blocks.10.attn.qkv.bias\",\n",
      "      \"backbone.blocks.10.attn.proj.bias\",\n",
      "      \"backbone.blocks.10.norm2.weight\",\n",
      "      \"backbone.blocks.10.norm2.bias\",\n",
      "      \"backbone.blocks.10.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.10.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81,\n",
      "    \"lr\": 4.86e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_11_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.10.attn.relative_position_bias_table\",\n",
      "      \"backbone.blocks.10.attn.sampling_offsets_k.2.weight\",\n",
      "      \"backbone.blocks.10.attn.sampling_scales_k.2.weight\",\n",
      "      \"backbone.blocks.10.attn.sampling_angles_k.2.weight\",\n",
      "      \"backbone.blocks.10.attn.sampling_offsets_v.2.weight\",\n",
      "      \"backbone.blocks.10.attn.sampling_scales_v.2.weight\",\n",
      "      \"backbone.blocks.10.attn.sampling_angles_v.2.weight\",\n",
      "      \"backbone.blocks.10.attn.qkv.weight\",\n",
      "      \"backbone.blocks.10.attn.proj.weight\",\n",
      "      \"backbone.blocks.10.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.10.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81,\n",
      "    \"lr\": 4.86e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_12_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.11.norm1.weight\",\n",
      "      \"backbone.blocks.11.norm1.bias\",\n",
      "      \"backbone.blocks.11.attn.rel_pos_h\",\n",
      "      \"backbone.blocks.11.attn.rel_pos_w\",\n",
      "      \"backbone.blocks.11.attn.qkv.bias\",\n",
      "      \"backbone.blocks.11.attn.proj.bias\",\n",
      "      \"backbone.blocks.11.norm2.weight\",\n",
      "      \"backbone.blocks.11.norm2.bias\",\n",
      "      \"backbone.blocks.11.mlp.fc1.bias\",\n",
      "      \"backbone.blocks.11.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9,\n",
      "    \"lr\": 5.4000000000000005e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_12_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.blocks.11.attn.qkv.weight\",\n",
      "      \"backbone.blocks.11.attn.proj.weight\",\n",
      "      \"backbone.blocks.11.mlp.fc1.weight\",\n",
      "      \"backbone.blocks.11.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9,\n",
      "    \"lr\": 5.4000000000000005e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.norm.weight\",\n",
      "      \"backbone.norm.bias\",\n",
      "      \"backbone.fpn1.0.bias\",\n",
      "      \"backbone.fpn1.1.ln.weight\",\n",
      "      \"backbone.fpn1.1.ln.bias\",\n",
      "      \"backbone.fpn1.3.bias\",\n",
      "      \"backbone.fpn2.0.bias\",\n",
      "      \"decode_head.conv_seg.bias\",\n",
      "      \"decode_head.psp_modules.0.1.bn.weight\",\n",
      "      \"decode_head.psp_modules.0.1.bn.bias\",\n",
      "      \"decode_head.psp_modules.1.1.bn.weight\",\n",
      "      \"decode_head.psp_modules.1.1.bn.bias\",\n",
      "      \"decode_head.psp_modules.2.1.bn.weight\",\n",
      "      \"decode_head.psp_modules.2.1.bn.bias\",\n",
      "      \"decode_head.psp_modules.3.1.bn.weight\",\n",
      "      \"decode_head.psp_modules.3.1.bn.bias\",\n",
      "      \"decode_head.bottleneck.bn.weight\",\n",
      "      \"decode_head.bottleneck.bn.bias\",\n",
      "      \"decode_head.lateral_convs.0.bn.weight\",\n",
      "      \"decode_head.lateral_convs.0.bn.bias\",\n",
      "      \"decode_head.lateral_convs.1.bn.weight\",\n",
      "      \"decode_head.lateral_convs.1.bn.bias\",\n",
      "      \"decode_head.lateral_convs.2.bn.weight\",\n",
      "      \"decode_head.lateral_convs.2.bn.bias\",\n",
      "      \"decode_head.fpn_convs.0.bn.weight\",\n",
      "      \"decode_head.fpn_convs.0.bn.bias\",\n",
      "      \"decode_head.fpn_convs.1.bn.weight\",\n",
      "      \"decode_head.fpn_convs.1.bn.bias\",\n",
      "      \"decode_head.fpn_convs.2.bn.weight\",\n",
      "      \"decode_head.fpn_convs.2.bn.bias\",\n",
      "      \"decode_head.fpn_bottleneck.bn.weight\",\n",
      "      \"decode_head.fpn_bottleneck.bn.bias\",\n",
      "      \"auxiliary_head.conv_seg.bias\",\n",
      "      \"auxiliary_head.convs.0.bn.weight\",\n",
      "      \"auxiliary_head.convs.0.bn.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"backbone.fpn1.0.weight\",\n",
      "      \"backbone.fpn1.3.weight\",\n",
      "      \"backbone.fpn2.0.weight\",\n",
      "      \"decode_head.conv_seg.weight\",\n",
      "      \"decode_head.psp_modules.0.1.conv.weight\",\n",
      "      \"decode_head.psp_modules.1.1.conv.weight\",\n",
      "      \"decode_head.psp_modules.2.1.conv.weight\",\n",
      "      \"decode_head.psp_modules.3.1.conv.weight\",\n",
      "      \"decode_head.bottleneck.conv.weight\",\n",
      "      \"decode_head.lateral_convs.0.conv.weight\",\n",
      "      \"decode_head.lateral_convs.1.conv.weight\",\n",
      "      \"decode_head.lateral_convs.2.conv.weight\",\n",
      "      \"decode_head.fpn_convs.0.conv.weight\",\n",
      "      \"decode_head.fpn_convs.1.conv.weight\",\n",
      "      \"decode_head.fpn_convs.2.conv.weight\",\n",
      "      \"decode_head.fpn_bottleneck.conv.weight\",\n",
      "      \"auxiliary_head.conv_seg.weight\",\n",
      "      \"auxiliary_head.convs.0.conv.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 12:29:59,082 - mmseg - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for decode_head.conv_seg.weight: copying a param with shape torch.Size([5, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 512, 1, 1]).\n",
      "size mismatch for decode_head.conv_seg.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "size mismatch for auxiliary_head.conv_seg.weight: copying a param with shape torch.Size([5, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 256, 1, 1]).\n",
      "size mismatch for auxiliary_head.conv_seg.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "unexpected key in source state_dict: backbone.blocks.0.PCM.0.weight, backbone.blocks.0.PCM.0.bias, backbone.blocks.0.PCM.1.weight, backbone.blocks.0.PCM.1.bias, backbone.blocks.0.PCM.1.running_mean, backbone.blocks.0.PCM.1.running_var, backbone.blocks.0.PCM.1.num_batches_tracked, backbone.blocks.0.PCM.3.weight, backbone.blocks.0.PCM.3.bias, backbone.blocks.0.attn.q_bias, backbone.blocks.0.attn.v_bias, backbone.blocks.0.attn.sampling_offsets.2.weight, backbone.blocks.0.attn.sampling_offsets.2.bias, backbone.blocks.0.attn.sampling_scales.2.weight, backbone.blocks.0.attn.sampling_scales.2.bias, backbone.blocks.0.attn.sampling_angles.2.weight, backbone.blocks.0.attn.sampling_angles.2.bias, backbone.blocks.1.PCM.0.weight, backbone.blocks.1.PCM.0.bias, backbone.blocks.1.PCM.1.weight, backbone.blocks.1.PCM.1.bias, backbone.blocks.1.PCM.1.running_mean, backbone.blocks.1.PCM.1.running_var, backbone.blocks.1.PCM.1.num_batches_tracked, backbone.blocks.1.PCM.3.weight, backbone.blocks.1.PCM.3.bias, backbone.blocks.1.attn.q_bias, backbone.blocks.1.attn.v_bias, backbone.blocks.1.attn.sampling_offsets.2.weight, backbone.blocks.1.attn.sampling_offsets.2.bias, backbone.blocks.1.attn.sampling_scales.2.weight, backbone.blocks.1.attn.sampling_scales.2.bias, backbone.blocks.1.attn.sampling_angles.2.weight, backbone.blocks.1.attn.sampling_angles.2.bias, backbone.blocks.2.PCM.0.weight, backbone.blocks.2.PCM.0.bias, backbone.blocks.2.PCM.1.weight, backbone.blocks.2.PCM.1.bias, backbone.blocks.2.PCM.1.running_mean, backbone.blocks.2.PCM.1.running_var, backbone.blocks.2.PCM.1.num_batches_tracked, backbone.blocks.2.PCM.3.weight, backbone.blocks.2.PCM.3.bias, backbone.blocks.2.attn.q_bias, backbone.blocks.2.attn.v_bias, backbone.blocks.3.PCM.0.weight, backbone.blocks.3.PCM.0.bias, backbone.blocks.3.PCM.1.weight, backbone.blocks.3.PCM.1.bias, backbone.blocks.3.PCM.1.running_mean, backbone.blocks.3.PCM.1.running_var, backbone.blocks.3.PCM.1.num_batches_tracked, backbone.blocks.3.PCM.3.weight, backbone.blocks.3.PCM.3.bias, backbone.blocks.3.attn.q_bias, backbone.blocks.3.attn.v_bias, backbone.blocks.3.attn.sampling_offsets.2.weight, backbone.blocks.3.attn.sampling_offsets.2.bias, backbone.blocks.3.attn.sampling_scales.2.weight, backbone.blocks.3.attn.sampling_scales.2.bias, backbone.blocks.3.attn.sampling_angles.2.weight, backbone.blocks.3.attn.sampling_angles.2.bias, backbone.blocks.4.PCM.0.weight, backbone.blocks.4.PCM.0.bias, backbone.blocks.4.PCM.1.weight, backbone.blocks.4.PCM.1.bias, backbone.blocks.4.PCM.1.running_mean, backbone.blocks.4.PCM.1.running_var, backbone.blocks.4.PCM.1.num_batches_tracked, backbone.blocks.4.PCM.3.weight, backbone.blocks.4.PCM.3.bias, backbone.blocks.4.attn.q_bias, backbone.blocks.4.attn.v_bias, backbone.blocks.4.attn.sampling_offsets.2.weight, backbone.blocks.4.attn.sampling_offsets.2.bias, backbone.blocks.4.attn.sampling_scales.2.weight, backbone.blocks.4.attn.sampling_scales.2.bias, backbone.blocks.4.attn.sampling_angles.2.weight, backbone.blocks.4.attn.sampling_angles.2.bias, backbone.blocks.5.PCM.0.weight, backbone.blocks.5.PCM.0.bias, backbone.blocks.5.PCM.1.weight, backbone.blocks.5.PCM.1.bias, backbone.blocks.5.PCM.1.running_mean, backbone.blocks.5.PCM.1.running_var, backbone.blocks.5.PCM.1.num_batches_tracked, backbone.blocks.5.PCM.3.weight, backbone.blocks.5.PCM.3.bias, backbone.blocks.5.attn.q_bias, backbone.blocks.5.attn.v_bias, backbone.blocks.6.PCM.0.weight, backbone.blocks.6.PCM.0.bias, backbone.blocks.6.PCM.1.weight, backbone.blocks.6.PCM.1.bias, backbone.blocks.6.PCM.1.running_mean, backbone.blocks.6.PCM.1.running_var, backbone.blocks.6.PCM.1.num_batches_tracked, backbone.blocks.6.PCM.3.weight, backbone.blocks.6.PCM.3.bias, backbone.blocks.6.attn.q_bias, backbone.blocks.6.attn.v_bias, backbone.blocks.6.attn.sampling_offsets.2.weight, backbone.blocks.6.attn.sampling_offsets.2.bias, backbone.blocks.6.attn.sampling_scales.2.weight, backbone.blocks.6.attn.sampling_scales.2.bias, backbone.blocks.6.attn.sampling_angles.2.weight, backbone.blocks.6.attn.sampling_angles.2.bias, backbone.blocks.7.PCM.0.weight, backbone.blocks.7.PCM.0.bias, backbone.blocks.7.PCM.1.weight, backbone.blocks.7.PCM.1.bias, backbone.blocks.7.PCM.1.running_mean, backbone.blocks.7.PCM.1.running_var, backbone.blocks.7.PCM.1.num_batches_tracked, backbone.blocks.7.PCM.3.weight, backbone.blocks.7.PCM.3.bias, backbone.blocks.7.attn.q_bias, backbone.blocks.7.attn.v_bias, backbone.blocks.7.attn.sampling_offsets.2.weight, backbone.blocks.7.attn.sampling_offsets.2.bias, backbone.blocks.7.attn.sampling_scales.2.weight, backbone.blocks.7.attn.sampling_scales.2.bias, backbone.blocks.7.attn.sampling_angles.2.weight, backbone.blocks.7.attn.sampling_angles.2.bias, backbone.blocks.8.PCM.0.weight, backbone.blocks.8.PCM.0.bias, backbone.blocks.8.PCM.1.weight, backbone.blocks.8.PCM.1.bias, backbone.blocks.8.PCM.1.running_mean, backbone.blocks.8.PCM.1.running_var, backbone.blocks.8.PCM.1.num_batches_tracked, backbone.blocks.8.PCM.3.weight, backbone.blocks.8.PCM.3.bias, backbone.blocks.8.attn.q_bias, backbone.blocks.8.attn.v_bias, backbone.blocks.9.PCM.0.weight, backbone.blocks.9.PCM.0.bias, backbone.blocks.9.PCM.1.weight, backbone.blocks.9.PCM.1.bias, backbone.blocks.9.PCM.1.running_mean, backbone.blocks.9.PCM.1.running_var, backbone.blocks.9.PCM.1.num_batches_tracked, backbone.blocks.9.PCM.3.weight, backbone.blocks.9.PCM.3.bias, backbone.blocks.9.attn.q_bias, backbone.blocks.9.attn.v_bias, backbone.blocks.9.attn.sampling_offsets.2.weight, backbone.blocks.9.attn.sampling_offsets.2.bias, backbone.blocks.9.attn.sampling_scales.2.weight, backbone.blocks.9.attn.sampling_scales.2.bias, backbone.blocks.9.attn.sampling_angles.2.weight, backbone.blocks.9.attn.sampling_angles.2.bias, backbone.blocks.10.PCM.0.weight, backbone.blocks.10.PCM.0.bias, backbone.blocks.10.PCM.1.weight, backbone.blocks.10.PCM.1.bias, backbone.blocks.10.PCM.1.running_mean, backbone.blocks.10.PCM.1.running_var, backbone.blocks.10.PCM.1.num_batches_tracked, backbone.blocks.10.PCM.3.weight, backbone.blocks.10.PCM.3.bias, backbone.blocks.10.attn.q_bias, backbone.blocks.10.attn.v_bias, backbone.blocks.10.attn.sampling_offsets.2.weight, backbone.blocks.10.attn.sampling_offsets.2.bias, backbone.blocks.10.attn.sampling_scales.2.weight, backbone.blocks.10.attn.sampling_scales.2.bias, backbone.blocks.10.attn.sampling_angles.2.weight, backbone.blocks.10.attn.sampling_angles.2.bias, backbone.blocks.11.PCM.0.weight, backbone.blocks.11.PCM.0.bias, backbone.blocks.11.PCM.1.weight, backbone.blocks.11.PCM.1.bias, backbone.blocks.11.PCM.1.running_mean, backbone.blocks.11.PCM.1.running_var, backbone.blocks.11.PCM.1.num_batches_tracked, backbone.blocks.11.PCM.3.weight, backbone.blocks.11.PCM.3.bias, backbone.blocks.11.attn.q_bias, backbone.blocks.11.attn.v_bias\n",
      "\n",
      "missing keys in source state_dict: backbone.blocks.0.attn.sampling_offsets_k.2.weight, backbone.blocks.0.attn.sampling_offsets_k.2.bias, backbone.blocks.0.attn.sampling_scales_k.2.weight, backbone.blocks.0.attn.sampling_scales_k.2.bias, backbone.blocks.0.attn.sampling_angles_k.2.weight, backbone.blocks.0.attn.sampling_angles_k.2.bias, backbone.blocks.0.attn.sampling_offsets_v.2.weight, backbone.blocks.0.attn.sampling_offsets_v.2.bias, backbone.blocks.0.attn.sampling_scales_v.2.weight, backbone.blocks.0.attn.sampling_scales_v.2.bias, backbone.blocks.0.attn.sampling_angles_v.2.weight, backbone.blocks.0.attn.sampling_angles_v.2.bias, backbone.blocks.0.attn.qkv.bias, backbone.blocks.1.attn.sampling_offsets_k.2.weight, backbone.blocks.1.attn.sampling_offsets_k.2.bias, backbone.blocks.1.attn.sampling_scales_k.2.weight, backbone.blocks.1.attn.sampling_scales_k.2.bias, backbone.blocks.1.attn.sampling_angles_k.2.weight, backbone.blocks.1.attn.sampling_angles_k.2.bias, backbone.blocks.1.attn.sampling_offsets_v.2.weight, backbone.blocks.1.attn.sampling_offsets_v.2.bias, backbone.blocks.1.attn.sampling_scales_v.2.weight, backbone.blocks.1.attn.sampling_scales_v.2.bias, backbone.blocks.1.attn.sampling_angles_v.2.weight, backbone.blocks.1.attn.sampling_angles_v.2.bias, backbone.blocks.1.attn.qkv.bias, backbone.blocks.2.attn.qkv.bias, backbone.blocks.3.attn.sampling_offsets_k.2.weight, backbone.blocks.3.attn.sampling_offsets_k.2.bias, backbone.blocks.3.attn.sampling_scales_k.2.weight, backbone.blocks.3.attn.sampling_scales_k.2.bias, backbone.blocks.3.attn.sampling_angles_k.2.weight, backbone.blocks.3.attn.sampling_angles_k.2.bias, backbone.blocks.3.attn.sampling_offsets_v.2.weight, backbone.blocks.3.attn.sampling_offsets_v.2.bias, backbone.blocks.3.attn.sampling_scales_v.2.weight, backbone.blocks.3.attn.sampling_scales_v.2.bias, backbone.blocks.3.attn.sampling_angles_v.2.weight, backbone.blocks.3.attn.sampling_angles_v.2.bias, backbone.blocks.3.attn.qkv.bias, backbone.blocks.4.attn.sampling_offsets_k.2.weight, backbone.blocks.4.attn.sampling_offsets_k.2.bias, backbone.blocks.4.attn.sampling_scales_k.2.weight, backbone.blocks.4.attn.sampling_scales_k.2.bias, backbone.blocks.4.attn.sampling_angles_k.2.weight, backbone.blocks.4.attn.sampling_angles_k.2.bias, backbone.blocks.4.attn.sampling_offsets_v.2.weight, backbone.blocks.4.attn.sampling_offsets_v.2.bias, backbone.blocks.4.attn.sampling_scales_v.2.weight, backbone.blocks.4.attn.sampling_scales_v.2.bias, backbone.blocks.4.attn.sampling_angles_v.2.weight, backbone.blocks.4.attn.sampling_angles_v.2.bias, backbone.blocks.4.attn.qkv.bias, backbone.blocks.5.attn.qkv.bias, backbone.blocks.6.attn.sampling_offsets_k.2.weight, backbone.blocks.6.attn.sampling_offsets_k.2.bias, backbone.blocks.6.attn.sampling_scales_k.2.weight, backbone.blocks.6.attn.sampling_scales_k.2.bias, backbone.blocks.6.attn.sampling_angles_k.2.weight, backbone.blocks.6.attn.sampling_angles_k.2.bias, backbone.blocks.6.attn.sampling_offsets_v.2.weight, backbone.blocks.6.attn.sampling_offsets_v.2.bias, backbone.blocks.6.attn.sampling_scales_v.2.weight, backbone.blocks.6.attn.sampling_scales_v.2.bias, backbone.blocks.6.attn.sampling_angles_v.2.weight, backbone.blocks.6.attn.sampling_angles_v.2.bias, backbone.blocks.6.attn.qkv.bias, backbone.blocks.7.attn.sampling_offsets_k.2.weight, backbone.blocks.7.attn.sampling_offsets_k.2.bias, backbone.blocks.7.attn.sampling_scales_k.2.weight, backbone.blocks.7.attn.sampling_scales_k.2.bias, backbone.blocks.7.attn.sampling_angles_k.2.weight, backbone.blocks.7.attn.sampling_angles_k.2.bias, backbone.blocks.7.attn.sampling_offsets_v.2.weight, backbone.blocks.7.attn.sampling_offsets_v.2.bias, backbone.blocks.7.attn.sampling_scales_v.2.weight, backbone.blocks.7.attn.sampling_scales_v.2.bias, backbone.blocks.7.attn.sampling_angles_v.2.weight, backbone.blocks.7.attn.sampling_angles_v.2.bias, backbone.blocks.7.attn.qkv.bias, backbone.blocks.8.attn.qkv.bias, backbone.blocks.9.attn.sampling_offsets_k.2.weight, backbone.blocks.9.attn.sampling_offsets_k.2.bias, backbone.blocks.9.attn.sampling_scales_k.2.weight, backbone.blocks.9.attn.sampling_scales_k.2.bias, backbone.blocks.9.attn.sampling_angles_k.2.weight, backbone.blocks.9.attn.sampling_angles_k.2.bias, backbone.blocks.9.attn.sampling_offsets_v.2.weight, backbone.blocks.9.attn.sampling_offsets_v.2.bias, backbone.blocks.9.attn.sampling_scales_v.2.weight, backbone.blocks.9.attn.sampling_scales_v.2.bias, backbone.blocks.9.attn.sampling_angles_v.2.weight, backbone.blocks.9.attn.sampling_angles_v.2.bias, backbone.blocks.9.attn.qkv.bias, backbone.blocks.10.attn.sampling_offsets_k.2.weight, backbone.blocks.10.attn.sampling_offsets_k.2.bias, backbone.blocks.10.attn.sampling_scales_k.2.weight, backbone.blocks.10.attn.sampling_scales_k.2.bias, backbone.blocks.10.attn.sampling_angles_k.2.weight, backbone.blocks.10.attn.sampling_angles_k.2.bias, backbone.blocks.10.attn.sampling_offsets_v.2.weight, backbone.blocks.10.attn.sampling_offsets_v.2.bias, backbone.blocks.10.attn.sampling_scales_v.2.weight, backbone.blocks.10.attn.sampling_scales_v.2.bias, backbone.blocks.10.attn.sampling_angles_v.2.weight, backbone.blocks.10.attn.sampling_angles_v.2.bias, backbone.blocks.10.attn.qkv.bias, backbone.blocks.11.attn.qkv.bias\n",
      "\n",
      "2022-11-24 12:29:59,116 - mmseg - INFO - Start running, host: smlm-workstation@smlmworkstation, work_dir: /home/smlm-workstation/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation/work_dirs/ViTAE_UperNet_Potsdam_512_MT_ER\n",
      "2022-11-24 12:29:59,117 - mmseg - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-11-24 12:29:59,117 - mmseg - INFO - workflow: [('train', 1)], max: 2000 iters\n",
      "2022-11-24 12:29:59,117 - mmseg - INFO - Checkpoints will be saved to /home/smlm-workstation/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation/work_dirs/ViTAE_UperNet_Potsdam_512_MT_ER by HardDiskBackend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create work_dir\u001b[39;00m\n\u001b[1;32m     16\u001b[0m mmcv\u001b[38;5;241m.\u001b[39mmkdir_or_exist(osp\u001b[38;5;241m.\u001b[39mabspath(cfg\u001b[38;5;241m.\u001b[39mwork_dir))\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain_segmentor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation/mmseg/apis/train.py:174\u001b[0m, in \u001b[0;36mtrain_segmentor\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mload_from:\n\u001b[1;32m    173\u001b[0m     runner\u001b[38;5;241m.\u001b[39mload_checkpoint(cfg\u001b[38;5;241m.\u001b[39mload_from)\n\u001b[0;32m--> 174\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vitae/lib/python3.8/site-packages/mmcv/runner/iter_based_runner.py:134\u001b[0m, in \u001b[0;36mIterBasedRunner.run\u001b[0;34m(self, data_loaders, workflow, max_iters, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iters:\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m             \u001b[43miter_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vitae/lib/python3.8/site-packages/mmcv/runner/iter_based_runner.py:61\u001b[0m, in \u001b[0;36mIterBasedRunner.train\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_loader)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.train_step() must return a dict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vitae/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py:75\u001b[0m, in \u001b[0;36mMMDataParallel.train_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound one of them on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscatter(inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation/mmseg/models/segmentors/base.py:139\u001b[0m, in \u001b[0;36mBaseSegmentor.train_step\u001b[0;34m(self, data_batch, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"The iteration step during training.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03mThis method defines an iteration step during training, except for the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m        averaging the logs.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_batch)\n\u001b[0;32m--> 139\u001b[0m loss, log_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    142\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m    143\u001b[0m     log_vars\u001b[38;5;241m=\u001b[39mlog_vars,\n\u001b[1;32m    144\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_metas\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/segmentation/ViTAE-Transformer-Remote-Sensing/SemanticSegmentation/mmseg/models/segmentors/base.py:208\u001b[0m, in \u001b[0;36mBaseSegmentor._parse_losses\u001b[0;34m(losses)\u001b[0m\n\u001b[1;32m    206\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m loss_value\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    207\u001b[0m         dist\u001b[38;5;241m.\u001b[39mall_reduce(loss_value\u001b[38;5;241m.\u001b[39mdiv_(dist\u001b[38;5;241m.\u001b[39mget_world_size()))\n\u001b[0;32m--> 208\u001b[0m     log_vars[loss_name] \u001b[38;5;241m=\u001b[39m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, log_vars\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from mmseg.datasets import build_dataset\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.apis import train_segmentor\n",
    "\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the detector\n",
    "model = build_segmentor(cfg.model)#.cuda().eval()\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "model.PALETTE = datasets[0].PALETTE\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_segmentor(model, datasets, cfg, distributed=False, validate=True, \n",
    "                meta=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\n",
    "#     '/home/smlm-workstation/segmentation/mmsegmentation/work_dirs/segformer_b1_adamW_16k/iter_16000.pth')\n",
    "# model['meta']['PALETTE'] = [[128, 255, 0], [0, 255, 255]]\n",
    "# torch.save(\n",
    "#         model, '/home/smlm-workstation/segmentation/mmsegmentation/work_dirs/segformer_b1_adamW_16k/iter_16000_palette.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekG__UfaH_OU"
   },
   "outputs": [],
   "source": [
    "# from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "# from skimage.io import imread, imshow, imsave\n",
    "# import os\n",
    "# model.cfg = cfg\n",
    "\n",
    "# # model = init_segmentor(\n",
    "# #     cfg, checkpoint='/home/smlm-workstation/segmentation/mmsegmentation/work_dirs/segformer_b1_adamW_16k/iter_16000_palette.pth')\n",
    "\n",
    "# im_list = [f'/home/smlm-workstation/segmentation/data/archive/mt_er/{fname}' for fname in os.listdir('/home/smlm-workstation/segmentation/data/archive/mt_er')]\n",
    "# i = 0\n",
    "# for im in im_list:\n",
    "#     img = mmcv.imread(im)\n",
    "#     img2 = mmcv.imread(im, flag='grayscale')\n",
    "#    #  img2 = imread(im, as_gray = True)\n",
    "#     result = inference_segmentor(model, img)\n",
    "#     mt, cl = np.zeros(shape=(img.shape[:2])), np.zeros(shape=(img.shape[:2]))\n",
    "#     mmt, mcl = np.array(result[0] == 0), np.array(result[0] == 1)\n",
    "#     mt[mmt] = img2[mmt].astype(np.uint8)\n",
    "#     cl[mcl] = img2[mcl].astype(np.uint8)\n",
    "#     imsave(\n",
    "#         f'/home/smlm-workstation/segmentation/data/results/segformerb1_transfer_reduce0_dice_256px_16k_MT_ER/{i}_MT.png', mt, check_contrast=False)\n",
    "#     imsave(\n",
    "#         f'/home/smlm-workstation/segmentation/data/results/segformerb1_transfer_reduce0_dice_256px_16k_MT_ER/{i}_ER.png', cl, check_contrast=False)  \n",
    "#     palette = [[128, 255, 0], [255, 0, 255]]\n",
    "#     # plt.figure(figsize=(12, 8))\n",
    "#     show_result_pyplot(model, img, result, palette, opacity=0.3, \n",
    "#                     out_file=f'/home/smlm-workstation/segmentation/data/results/segformerb1_transfer_reduce0_dice_256px_16k_MT_ER/{i}_segmap.png')\n",
    "#     i += 1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMSegmentation_SMLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "da41283624f67442485b423f387d5493928c0694ee1fbac3824d3ee1c9f79beb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
